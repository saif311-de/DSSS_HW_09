{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the Telegram Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully.\n",
      "Bot is running...\n",
      "Message from 7210772876: Tell me something about Lions\n",
      "Message from 7210772876: Thanks\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "\n",
    "# Apply the patch for running in environments with existing event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "MODEL_PATH = r\"U:\\E.Mobility\\WS 2024-25\\Data Science Survival Skills\\Homeworks\\H9\\TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load the LLM model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "TOKEN = \"7667178361:AAFRZFZqXl6kAYa5HXSSK_HcJixusnYiXbw\"\n",
    "\n",
    "# Function to handle incoming messages\n",
    "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    user_message = update.message.text.strip()\n",
    "    user_id = update.effective_user.id\n",
    "\n",
    "    print(f\"Message from {user_id}: {user_message}\")\n",
    "\n",
    "    # Add system instruction to guide the LLM\n",
    "    prompt = (\n",
    "        \"You are a knowledgeable assistant. Please provide detailed and accurate \"\n",
    "        \"information about the topic asked.\\n\\nUser: \" + user_message + \"\\nAssistant:\"\n",
    "    )\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    try:\n",
    "        # Generate response with controlled decoding\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=200,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the prompt and ensure only the assistant's direct response is sent\n",
    "        if \"Assistant:\" in full_response:\n",
    "            bot_response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "        else:\n",
    "            bot_response = full_response.strip()\n",
    "\n",
    "        # Stop at the first instance of \"User:\" in the response (if hallucinated)\n",
    "        if \"User:\" in bot_response:\n",
    "            bot_response = bot_response.split(\"User:\")[0].strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM inference: {e}\")\n",
    "        bot_response = \"Sorry, I encountered an error while processing your request.\"\n",
    "\n",
    "    # Send the response back to the user\n",
    "    await update.message.reply_text(bot_response)\n",
    "\n",
    "# Function to handle the /start command\n",
    "async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\n",
    "        \"Welcome to Noob1.0! You can send me a message, and I'll try to respond.\"\n",
    "    )\n",
    "\n",
    "# Main function to start the bot\n",
    "async def main():\n",
    "    # Initialize the bot application\n",
    "    app = ApplicationBuilder().token(TOKEN).build()\n",
    "\n",
    "    # Add command and message handlers\n",
    "    app.add_handler(CommandHandler(\"start\", start_command))\n",
    "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
    "\n",
    "    # Start the bot\n",
    "    print(\"Bot is running...\")\n",
    "    await app.run_polling()\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Screenshot Purpose\n",
    "To add screenshots to homework file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "# from telegram import Update\n",
    "# from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "\n",
    "# # Apply the patch for running in environments with existing event loops\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# TOKEN = \"7667178361:AAFRZFZqXl6kAYa5HXSSK_HcJixusnYiXbw\"\n",
    "\n",
    "# async def handle_message(update: Update, \n",
    "#                          context: ContextTypes.DEFAULT_TYPE):\n",
    "#     user_message = update.message.text.strip()\n",
    "#     user_id = update.effective_user.id\n",
    "\n",
    "#     print(f\"Message from {user_id}: {user_message}\")\n",
    "\n",
    "# async def start_command(update: Update, \n",
    "#                         context: ContextTypes.DEFAULT_TYPE):\n",
    "#     await update.message.reply_text(\n",
    "#         \"Welcome to Noob1.0! You can send me a message,\\\n",
    "#             and I'll try to respond.\"\n",
    "#     )\n",
    "# async def main():\n",
    "#     app = ApplicationBuilder().token(TOKEN).build()\n",
    "\n",
    "#     app.add_handler(CommandHandler(\"start\", start_command))\n",
    "#     app.add_handler(MessageHandler(\n",
    "#         filters.TEXT & ~filters.COMMAND, handle_message))\n",
    "#     print(\"Bot is running...\")\n",
    "#     await app.run_polling()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# MODEL_PATH = r\"U:\\E.Mobility\\WS 2024-25\\Data Science Survival \\\n",
    "#                 Skills\\Homeworks\\H9\\TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# # Load the LLM model and tokenizer\n",
    "# print(\"Loading model...\")\n",
    "# try:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "#     device = torch.device(\n",
    "#         \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     )\n",
    "#     model.to(device)\n",
    "#     print(\"Model loaded successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")\n",
    "#     exit()\n",
    "\n",
    "# def generate_response(user_message: str):\n",
    "#     prompt = (\n",
    "#         \"You are a knowledgeable assistant. Please \"\n",
    "#         \"provide detailed and accurate information about \"\n",
    "#         \"the topic asked.\\n\\nUser: \" + user_message + \"\\nAssistant:\"\n",
    "#     )\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#     try:\n",
    "#         outputs = model.generate(inputs, max_length=200, \n",
    "#                                  num_return_sequences=1,do_sample=True, \n",
    "#                                  temperature=0.7, top_k=50, top_p=0.9)\n",
    "#         full_response = tokenizer.decode(\n",
    "#             outputs[0], skip_special_tokens=True)\n",
    "#         bot_response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "#         if \"User:\" in bot_response:\n",
    "#             bot_response = bot_response.split(\"User:\")[0].strip()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during LLM inference: {e}\")\n",
    "#         bot_response = (\n",
    "#             \"Sorry, I encountered an error while \"\n",
    "#             \"processing your request.\"\n",
    "#         )\n",
    "#     return bot_response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
